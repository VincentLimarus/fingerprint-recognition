{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be44000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory = \"../data/raw\"\n",
    "\n",
    "if not directory:\n",
    "    directory = input(\"Input raw dataset directory:\")\n",
    "\n",
    "files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "\n",
    "count = 1\n",
    "if files:\n",
    "    curr = files[0].split(\"_\")[0]\n",
    "else:\n",
    "    print(\"No files found in the directory.\")\n",
    "    exit()\n",
    "\n",
    "for file in files:\n",
    "    file_split = file.split(\"_\")\n",
    "    if file_split[0] != curr:\n",
    "        curr = file_split[0]\n",
    "        count += 1\n",
    "    \n",
    "    new_file_name = \"_\".join(file_split[1:])\n",
    "    new_file_name = str(count).zfill(3) + \"_\" + new_file_name\n",
    "    \n",
    "    old_path = os.path.join(directory, file)\n",
    "    new_path = os.path.join(directory, new_file_name)\n",
    "    \n",
    "    os.rename(old_path, new_path)\n",
    "    print(f\"Successfully changed file name from ({file}) to ({new_file_name})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d21391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def skeletonize(img):\n",
    "    \"\"\"Alternative skeletonization implementation without ximgproc\"\"\"\n",
    "    skel = np.zeros(img.shape, np.uint8)\n",
    "    element = cv2.getStructuringElement(cv2.MORPH_CROSS, (3,3))\n",
    "    while True:\n",
    "        open_img = cv2.morphologyEx(img, cv2.MORPH_OPEN, element)\n",
    "        temp = cv2.subtract(img, open_img)\n",
    "        eroded = cv2.erode(img, element)\n",
    "        skel = cv2.bitwise_or(skel, temp)\n",
    "        img = eroded.copy()\n",
    "        if cv2.countNonZero(img) == 0:\n",
    "            break\n",
    "    return skel\n",
    "\n",
    "def process_fingerprint(image_path, output_path, target_size=None, upscale_factor=1.0):\n",
    "    # Read image\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Could not read image: {image_path}\")\n",
    "    \n",
    "    # Resize if needed\n",
    "    if target_size:\n",
    "        image = cv2.resize(image, target_size, interpolation=cv2.INTER_LANCZOS4)\n",
    "    elif upscale_factor != 1.0:\n",
    "        h, w = image.shape\n",
    "        image = cv2.resize(image, (int(w*upscale_factor), int(h*upscale_factor)), \n",
    "                         interpolation=cv2.INTER_LANCZOS4)\n",
    "    \n",
    "    # 1. Contrast enhancement\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    enhanced = clahe.apply(image)\n",
    "    \n",
    "    # 2. Noise reduction\n",
    "    denoised = cv2.bilateralFilter(enhanced, 9, 75, 75)\n",
    "    \n",
    "    # 3. Adaptive thresholding\n",
    "    thresh = cv2.adaptiveThreshold(denoised, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                cv2.THRESH_BINARY_INV, 21, 7)\n",
    "    \n",
    "    # 4. Morphological operations\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    morph = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "    morph = cv2.morphologyEx(morph, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "    \n",
    "    # 5. Skeletonization (using alternative method)\n",
    "    skeleton = skeletonize(morph)\n",
    "    \n",
    "    # 6. Final inversion and saving\n",
    "    result = cv2.bitwise_not(skeleton)\n",
    "    cv2.imwrite(output_path, result)\n",
    "\n",
    "def batch_process_fingerprints(input_folder, output_folder, upscale=False):\n",
    "    \"\"\"\n",
    "    Process all fingerprints in a folder\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    files = [f for f in os.listdir(input_folder) if f.lower().endswith('.tif')]\n",
    "    \n",
    "    for filename in tqdm(files, desc=\"Processing Fingerprints\"):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        \n",
    "        try:\n",
    "            if upscale:\n",
    "                process_fingerprint(input_path, output_path, upscale_factor=2.0)\n",
    "            else:\n",
    "                process_fingerprint(input_path, output_path)\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError processing {filename}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    INPUT_FOLDER = \"../data/raw\"\n",
    "    OUTPUT_FOLDER = \"../data/processed\"\n",
    "    UPSCALE_IMAGES = True  # Set to True for 2x upscaling\n",
    "    \n",
    "    print(\"Starting fingerprint processing...\")\n",
    "    batch_process_fingerprints(INPUT_FOLDER, OUTPUT_FOLDER, upscale=UPSCALE_IMAGES)\n",
    "    print(\"\\nProcessing completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135fb8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)  # Ensures the same split every time you run the code\n",
    "\n",
    "# Define paths\n",
    "dataset_folder = \"../data/processed\"\n",
    "train_folder = \"../data/final/train\"\n",
    "test_folder = \"../data/final/test\"\n",
    "\n",
    "# Create train and test folders\n",
    "os.makedirs(train_folder, exist_ok=True)\n",
    "os.makedirs(test_folder, exist_ok=True)\n",
    "\n",
    "# Get all image files and shuffle them\n",
    "all_files = [f for f in os.listdir(dataset_folder) if f.endswith('.tif')]\n",
    "random.shuffle(all_files)  # Shuffle to ensure random distribution\n",
    "\n",
    "# Calculate split index (80% train, 20% test)\n",
    "split_idx = int(0.8 * len(all_files))\n",
    "train_files = all_files[:split_idx]  # First 80% for training\n",
    "test_files = all_files[split_idx:]   # Remaining 20% for testing\n",
    "\n",
    "print(f\"Found {len(all_files)} total fingerprint images\")\n",
    "print(f\"- Training images: {len(train_files)} (80%)\")\n",
    "print(f\"- Test images: {len(test_files)} (20%)\")\n",
    "\n",
    "# Function to copy files\n",
    "def copy_files(file_list, destination_folder):\n",
    "    for file in tqdm(file_list, desc=f\"Copying to {os.path.basename(destination_folder)}\"):\n",
    "        src = os.path.join(dataset_folder, file)\n",
    "        dest = os.path.join(destination_folder, file)\n",
    "        shutil.copy2(src, dest)\n",
    "\n",
    "# Copy files to respective folders\n",
    "copy_files(train_files, train_folder)\n",
    "copy_files(test_files, test_folder)\n",
    "\n",
    "print(\"\\nDataset organization completed:\")\n",
    "print(f\"- Training set: {len(train_files)} images (copied to {train_folder})\")\n",
    "print(f\"- Test set: {len(test_files)} images (copied to {test_folder})\")\n",
    "print(f\"Original files remain intact in {dataset_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17a267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Path configuration\n",
    "test_dir = \"../data/final/test\"\n",
    "\n",
    "def apply_block_damage_smart(image, block_size=80, num_blocks=7):\n",
    "    \"\"\"Apply white block damage only on fingerprint area.\"\"\"\n",
    "    damaged = np.copy(image)\n",
    "    height, width = image.shape\n",
    "    mask = image < 250\n",
    "    ys, xs = np.where(mask)\n",
    "\n",
    "    if len(xs) == 0:\n",
    "        return damaged\n",
    "\n",
    "    for _ in range(num_blocks):\n",
    "        idx = random.randint(0, len(xs) - 1)\n",
    "        x_center, y_center = xs[idx], ys[idx]\n",
    "        x1 = max(0, x_center - block_size // 2)\n",
    "        y1 = max(0, y_center - block_size // 2)\n",
    "        x2 = min(width, x1 + block_size)\n",
    "        y2 = min(height, y1 + block_size)\n",
    "        damaged[y1:y2, x1:x2] = 255\n",
    "    return damaged\n",
    "\n",
    "def apply_blur_damage(image, block_size=60, num_blocks=5):\n",
    "    damaged = np.copy(image)\n",
    "    height, width = image.shape\n",
    "\n",
    "    for _ in range(num_blocks):\n",
    "        x = random.randint(0, width - block_size)\n",
    "        y = random.randint(0, height - block_size)\n",
    "        roi = damaged[y:y+block_size, x:x+block_size]\n",
    "        blurred = cv2.GaussianBlur(roi, (11, 11), 0)\n",
    "        damaged[y:y+block_size, x:x+block_size] = blurred\n",
    "    return damaged\n",
    "\n",
    "def apply_elliptical_noise(image, num_ellipses=5):\n",
    "    damaged = np.copy(image)\n",
    "    height, width = image.shape\n",
    "\n",
    "    for _ in range(num_ellipses):\n",
    "        center = (\n",
    "            random.randint(0, width),\n",
    "            random.randint(0, height)\n",
    "        )\n",
    "        axes = (\n",
    "            random.randint(20, 60),\n",
    "            random.randint(10, 30)\n",
    "        )\n",
    "        angle = random.randint(0, 180)\n",
    "        cv2.ellipse(damaged, center, axes, angle, 0, 360, 255, -1)\n",
    "    return damaged\n",
    "\n",
    "def apply_combined_damage(image):\n",
    "    \"\"\"Apply all three types of damage sequentially.\"\"\"\n",
    "    image = apply_block_damage_smart(image)\n",
    "    image = apply_blur_damage(image)\n",
    "    image = apply_elliptical_noise(image)\n",
    "    return image\n",
    "\n",
    "# Get all test images (.tif files)\n",
    "test_images = [f for f in os.listdir(test_dir) if f.endswith(\".tif\")]\n",
    "print(f\"Found {len(test_images)} test images to process\")\n",
    "\n",
    "# Process and overwrite originals\n",
    "for filename in tqdm(test_images, desc=\"Applying combined damage\"):\n",
    "    image_path = os.path.join(test_dir, filename)\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    if image is not None:\n",
    "        damaged_image = apply_combined_damage(image)\n",
    "        cv2.imwrite(image_path, damaged_image)\n",
    "\n",
    "print(f\"\\nOverwritten {len(test_images)} images with combined damage in {test_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5cb04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(len(base_model.layers))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
